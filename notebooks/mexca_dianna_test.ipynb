{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dcce5d",
   "metadata": {},
   "source": [
    "<img width=\"150\" alt=\"Logo_ER10\" src=\"https://user-images.githubusercontent.com/3244249/151994514-b584b984-a148-4ade-80ee-0f88b0aefa45.png\">\n",
    "\n",
    "### Model Interpretation for Pretrained ImageNet Model using RISE\n",
    "\n",
    "This notebook demonstrates how to apply the RISE explainability method on pretrained ImageNet model using a bee image. It visualizes the relevance scores for all pixels/super-pixels by displaying them on the image.<br>\n",
    "\n",
    "[RISE](http://bmvc2018.org/contents/papers/1064.pdf) is short for Randomized Input Sampling for Explanation of Black-box Models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs.<br>\n",
    "\n",
    "\n",
    "#### Requirments:\n",
    "\n",
    "Install the required packages as:\n",
    "\n",
    "`pip install python<3.11 dianna mexca[all] opencv-python mediapipe`\n",
    "\n",
    "Download the `test_mediapipe.py` script from https://github.com/mexca/mexca/tree/dianna-demo-experiments/dianna-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d063c",
   "metadata": {},
   "source": [
    "#### Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5da63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "if running_in_colab:\n",
    "    # install dianna\n",
    "    !python3 -m pip install dianna[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767530b",
   "metadata": {},
   "source": [
    "#### 0 -  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84b24cc-bee8-4a5b-b86f-bf2a2af652b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable warnings related to versions of tf\n",
    "import numpy as np\n",
    "\n",
    "# keras model and preprocessing tools\n",
    "# from keras import backend as K\n",
    "\n",
    "# dianna library for explanation\n",
    "import dianna\n",
    "from dianna import visualization\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from mexca.video.extraction import MEFARG\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# for face detection and cropping\n",
    "import cv2\n",
    "from test_mediapipe import FaceDetector\n",
    "\n",
    "# for loading the AUs codes\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52107bc8",
   "metadata": {},
   "source": [
    "#### 1 - Loading the model and the dataset\n",
    "Loads pretrained ImageNet model and the image to be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee4d03",
   "metadata": {},
   "source": [
    "Initialize the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f068783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, device = torch.device(\"cpu\")):\n",
    "        # K.set_learning_phase(0)\n",
    "        self.model = MEFARG.from_pretrained(\n",
    "            \"mexca/mefarg-open-graph-au-resnet50-stage-2\"\n",
    "        ).to(device)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def run_on_batch(self, x):\n",
    "        if len(x.shape) == 4:\n",
    "            x_trans = torch.stack([self.transform(img) for img in x])\n",
    "        elif len(x.shape) == 3:\n",
    "            x_trans = self.transform(x)[None, :, :, :]\n",
    "        \n",
    "        # Move the tensor to the same device as the model\n",
    "        x_trans = x_trans.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(x_trans)\n",
    "        return predictions.detach().cpu().squeeze() # needs to be on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63f5a25c-035c-4362-ada4-b905aafd83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(device = torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae9db2-2f3e-4ce8-90a8-9d71ccc23749",
   "metadata": {},
   "source": [
    "##### 1.2 - Read an image and crop it using FaceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea830488-fb67-488e-9777-b4d63f8efcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732894712.133328 3171081 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1732894712.179533 1624989 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n",
      "W0000 00:00:1732894712.182577 1624985 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The photo of shape (1155, 1239, 3) is cropped to a photo of shape (397, 397, 3)\n"
     ]
    }
   ],
   "source": [
    "path_to_photo = \"/data/mexca_dianna_storage/demo_mexca.png\"\n",
    "frame = cv2.imread(path_to_photo)\n",
    "detector = FaceDetector(confidence_threshold = 0.8, device = \"cuda\")\n",
    "faces, detection_time, inference_time, was_processed = detector.process_frame(frame, 1)\n",
    "\n",
    "x = faces[0][\"crop\"]\n",
    "print(f\"The photo of shape {frame.shape} is cropped to a photo of shape {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7c8cb-db40-4049-84b9-55ee840ca73c",
   "metadata": {},
   "source": [
    "Run the model on the cropped photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "328ff827-2eb6-4623-9695-04d6265bbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.run_on_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097419cd",
   "metadata": {},
   "source": [
    "#### 2 - Compute and visualize the relevance scores\n",
    "Compute the pixel relevance scores using RISE and visualize them on the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65e16c",
   "metadata": {},
   "source": [
    "RISE masks random portions of the input image and passes the masked image through the model — the masked portion that decreases accuracy the most is the most “important” portion.<br>\n",
    "To call the explainer and generate relevance scores map, the user need to specifiy the number of masks being randomly generated (`n_masks`), the resolution of features in masks (`feature_res`) and for each mask and each feature in the image, the probability of being kept unmasked (`p_keep`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "714d95e6-62f1-4e11-9867-9c808fb1aeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "71fb94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [00:10<00:00,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.9 s, sys: 6.99 s, total: 40.9 s\n",
      "Wall time: 17.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "relevances = dianna.explain_image(model.run_on_batch, x, method=\"RISE\",\n",
    "                                labels=[i for i in range(41)],\n",
    "                                n_masks=1000, feature_res=6, p_keep=.1,\n",
    "                                axis_labels={2: 'channels'}, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248a326",
   "metadata": {},
   "source": [
    "Make predictions and select the top prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d126c8c4-402a-41de-b4c0-aa71a6968ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_name(idx):\n",
    "    au_list = np.array(\n",
    "        [1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 32, 38, 39]\n",
    "    )\n",
    "    with open('./AUs_codes.yaml') as f:\n",
    "        au_codes = yaml.load(f, Loader=yaml.FullLoader)[\"facial_action_units\"]\n",
    "    return au_codes.get(au_list[idx])['facs_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fee6fc",
   "metadata": {},
   "source": [
    "Visualize the relevance scores for the predicted class on top of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b16f6c59-25c5-428c-beb6-d523a13874db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction id 4: facs_name Cheek raiser\n",
      "prediction id 9: facs_name Lip corner puller\n",
      "prediction id 2: facs_name Brow lowerer\n",
      "prediction id 5: facs_name Lid tightener\n"
     ]
    }
   ],
   "source": [
    "predictions = model.run_on_batch(x).numpy()\n",
    "prediction_ids = np.array([4, 9, 2, 5])\n",
    "for idx in prediction_ids:\n",
    "    print(f\"prediction id {idx}: facs_name {class_name(idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8230bdb2-620a-434a-905c-9cd7c381d8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(x).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265f1f3-f65e-482a-b0f3-e871b2b1290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_idx in prediction_ids:\n",
    "    print(f'Explanation for `{class_name(class_idx)}` ({predictions[class_idx]})')\n",
    "    visualization.plot_image(relevances[class_idx], x/255.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e62e0e-f166-4dd5-a70c-ebe879e45c50",
   "metadata": {},
   "source": [
    "#### 3 - Conclusions\n",
    "The relevance scores are generated by passing multiple randomly masked inputs to the black-box model and averaging their scores. The idea behind this is that whenever a mask preserves important parts of the image it gets higher score. <br>\n",
    "\n",
    "The example here shows that the RISE method evaluates the relevance of each pixel/super pixel to the classification. Pixels characterizing the bee are highlighted by the XAI approach, which gives an intuition on how the model classifies the image. The results are reasonable, based on the human visual preception of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187b0a7-5835-4aa4-acaa-f690940fa1f4",
   "metadata": {},
   "source": [
    "#### 4 - Repeat the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb457d6-40a8-4fd7-bd22-855570461765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61414e2a-f9fd-4c03-9e59-c94cf5861dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 took 112.438 seconds\n",
      "Differences for Cheek raiser is 0.039\n",
      "Differences for Lip corner puller is 0.035\n",
      "Differences for Brow lowerer is 0.034\n",
      "Differences for Lid tightener is 0.033\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 112.555 seconds\n",
      "Differences for Cheek raiser is 0.047\n",
      "Differences for Lip corner puller is 0.042\n",
      "Differences for Brow lowerer is 0.040\n",
      "Differences for Lid tightener is 0.040\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 took 111.210 seconds\n",
      "Differences for Cheek raiser is 0.036\n",
      "Differences for Lip corner puller is 0.033\n",
      "Differences for Brow lowerer is 0.026\n",
      "Differences for Lid tightener is 0.028\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 took 112.270 seconds\n",
      "Differences for Cheek raiser is 0.038\n",
      "Differences for Lip corner puller is 0.034\n",
      "Differences for Brow lowerer is 0.030\n",
      "Differences for Lid tightener is 0.030\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 took 113.037 seconds\n",
      "Differences for Cheek raiser is 0.037\n",
      "Differences for Lip corner puller is 0.034\n",
      "Differences for Brow lowerer is 0.029\n",
      "Differences for Lid tightener is 0.031\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "first_relevances = relevances\n",
    "\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    relevances = dianna.explain_image(model.run_on_batch, x, method=\"RISE\",\n",
    "                                      labels=[i for i in range(41)],\n",
    "                                      n_masks=1000, feature_res=6, p_keep=.1,\n",
    "                                      axis_labels={2: 'channels'}, batch_size=10)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Iteration {i} took {elapsed_time:.3f} seconds\")\n",
    "    # compare the MAE of each iteration compared to the first one above\n",
    "    for class_idx in prediction_ids:\n",
    "        diff_relevances = np.mean(np.abs(relevances[class_idx] - first_relevances[class_idx]))\n",
    "        print(f'Differences for {class_name(class_idx)} is {diff_relevances:.3f}')\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9501549-aafa-4ad3-b0f5-295904752627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
