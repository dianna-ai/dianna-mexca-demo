{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69dcce5d",
   "metadata": {},
   "source": [
    "<img width=\"150\" alt=\"Logo_ER10\" src=\"https://user-images.githubusercontent.com/3244249/151994514-b584b984-a148-4ade-80ee-0f88b0aefa45.png\">\n",
    "\n",
    "### Model Interpretation for Pretrained ImageNet Model using RISE\n",
    "\n",
    "This notebook demonstrates how to apply the RISE explainability method on pretrained ImageNet model using a bee image. It visualizes the relevance scores for all pixels/super-pixels by displaying them on the image.<br>\n",
    "\n",
    "[RISE](http://bmvc2018.org/contents/papers/1064.pdf) is short for Randomized Input Sampling for Explanation of Black-box Models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs.<br>\n",
    "\n",
    "\n",
    "#### Requirments:\n",
    "\n",
    "Install the required packages as:\n",
    "\n",
    "`pip install python<3.11 dianna mexca[all] opencv-python mediapipe`\n",
    "\n",
    "Download the `test_mediapipe.py` script from https://github.com/mexca/mexca/tree/dianna-demo-experiments/dianna-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d063c",
   "metadata": {},
   "source": [
    "#### Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5da63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "if running_in_colab:\n",
    "    # install dianna\n",
    "    !python3 -m pip install dianna[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767530b",
   "metadata": {},
   "source": [
    "#### 0 -  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f84b24cc-bee8-4a5b-b86f-bf2a2af652b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 17:13:24.724104: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 17:13:24.757239: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-22 17:13:24.757272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-22 17:13:24.758165: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 17:13:24.764548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 17:13:25.510593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # disable warnings related to versions of tf\n",
    "import numpy as np\n",
    "\n",
    "# keras model and preprocessing tools\n",
    "# from keras import backend as K\n",
    "\n",
    "# dianna library for explanation\n",
    "import dianna\n",
    "from dianna import visualization\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from mexca.video.extraction import MEFARG\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# for face detection and cropping\n",
    "import cv2\n",
    "from test_mediapipe import FaceDetector\n",
    "\n",
    "# for loading the AUs codes\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52107bc8",
   "metadata": {},
   "source": [
    "#### 1 - Loading the model and the dataset\n",
    "Loads pretrained ImageNet model and the image to be explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee4d03",
   "metadata": {},
   "source": [
    "Initialize the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f068783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, device = torch.device(\"cpu\")):\n",
    "        # K.set_learning_phase(0)\n",
    "        self.model = MEFARG.from_pretrained(\n",
    "            \"mexca/mefarg-open-graph-au-resnet50-stage-2\"\n",
    "        )#.to(device)\n",
    "        self.model.eval()\n",
    "        self.input_size = (224, 224)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def run_on_batch(self, x):\n",
    "        if len(x.shape) == 4:\n",
    "            x_trans = torch.stack([self.transform(img) for img in x])\n",
    "        elif len(x.shape) == 3:\n",
    "            x_trans = self.transform(x)[None, :, :, :]\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(x_trans)\n",
    "        return predictions.detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4ec473",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae9db2-2f3e-4ce8-90a8-9d71ccc23749",
   "metadata": {},
   "source": [
    "##### 1.2 - Read an image and crop it using FaceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea830488-fb67-488e-9777-b4d63f8efcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732292007.302469 3171081 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1732292007.377442 3171177 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA A10/PCIe/SSE2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1732292007.381526 3171174 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The photo of shape (1155, 1239, 3) is cropped to a photo of shape (397, 397, 3)\n"
     ]
    }
   ],
   "source": [
    "path_to_photo = \"/data/mexca_dianna_storage/demo_mexca.png\"\n",
    "frame = cv2.imread(path_to_photo)\n",
    "detector = FaceDetector(confidence_threshold = 0.8, device = \"cuda\")\n",
    "faces, detection_time, inference_time, was_processed = detector.process_frame(frame, 1)\n",
    "\n",
    "x = faces[0][\"crop\"]\n",
    "print(f\"The photo of shape {frame.shape} is cropped to a photo of shape {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca7c8cb-db40-4049-84b9-55ee840ca73c",
   "metadata": {},
   "source": [
    "Run the model on the cropped photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae618677-784b-4674-8e92-8897e9d13eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.run_on_batch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097419cd",
   "metadata": {},
   "source": [
    "#### 2 - Compute and visualize the relevance scores\n",
    "Compute the pixel relevance scores using RISE and visualize them on the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65e16c",
   "metadata": {},
   "source": [
    "RISE masks random portions of the input image and passes the masked image through the model — the masked portion that decreases accuracy the most is the most “important” portion.<br>\n",
    "To call the explainer and generate relevance scores map, the user need to specifiy the number of masks being randomly generated (`n_masks`), the resolution of features in masks (`feature_res`) and for each mask and each feature in the image, the probability of being kept unmasked (`p_keep`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714d95e6-62f1-4e11-9867-9c808fb1aeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fb94af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 3s, sys: 5min 21s, total: 17min 25s\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "relevances = dianna.explain_image(model.run_on_batch, x, method=\"RISE\",\n",
    "                                labels=[i for i in range(41)],\n",
    "                                n_masks=1000, feature_res=6, p_keep=.1,\n",
    "                                axis_labels={2: 'channels'}, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248a326",
   "metadata": {},
   "source": [
    "Make predictions and select the top prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d126c8c4-402a-41de-b4c0-aa71a6968ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_name(idx):\n",
    "    au_list = np.array(\n",
    "        [1, 2, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 32, 38, 39]\n",
    "    )\n",
    "    with open('./AUs_codes.yaml') as f:\n",
    "        au_codes = yaml.load(f, Loader=yaml.FullLoader)[\"facial_action_units\"]\n",
    "    return au_codes.get(au_list[idx])['facs_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fee6fc",
   "metadata": {},
   "source": [
    "Visualize the relevance scores for the predicted class on top of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b16f6c59-25c5-428c-beb6-d523a13874db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction id 4: facs_name Cheek raiser\n",
      "prediction id 9: facs_name Lip corner puller\n",
      "prediction id 2: facs_name Brow lowerer\n",
      "prediction id 5: facs_name Lid tightener\n"
     ]
    }
   ],
   "source": [
    "predictions = model.run_on_batch(x).numpy()\n",
    "prediction_ids = np.array([4, 9, 2, 5])\n",
    "for idx in prediction_ids:\n",
    "    print(f\"prediction id {idx}: facs_name {class_name(idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8230bdb2-620a-434a-905c-9cd7c381d8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(x).numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265f1f3-f65e-482a-b0f3-e871b2b1290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_idx in prediction_ids:\n",
    "    print(f'Explanation for `{class_name(class_idx)}` ({predictions[class_idx]})')\n",
    "    visualization.plot_image(relevances[class_idx], x/255.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e62e0e-f166-4dd5-a70c-ebe879e45c50",
   "metadata": {},
   "source": [
    "#### 3 - Conclusions\n",
    "The relevance scores are generated by passing multiple randomly masked inputs to the black-box model and averaging their scores. The idea behind this is that whenever a mask preserves important parts of the image it gets higher score. <br>\n",
    "\n",
    "The example here shows that the RISE method evaluates the relevance of each pixel/super pixel to the classification. Pixels characterizing the bee are highlighted by the XAI approach, which gives an intuition on how the model classifies the image. The results are reasonable, based on the human visual preception of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187b0a7-5835-4aa4-acaa-f690940fa1f4",
   "metadata": {},
   "source": [
    "#### 4 - Repeat the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb457d6-40a8-4fd7-bd22-855570461765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61414e2a-f9fd-4c03-9e59-c94cf5861dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 took 112.438 seconds\n",
      "Differences for Cheek raiser is 0.039\n",
      "Differences for Lip corner puller is 0.035\n",
      "Differences for Brow lowerer is 0.034\n",
      "Differences for Lid tightener is 0.033\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 took 112.555 seconds\n",
      "Differences for Cheek raiser is 0.047\n",
      "Differences for Lip corner puller is 0.042\n",
      "Differences for Brow lowerer is 0.040\n",
      "Differences for Lid tightener is 0.040\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 took 111.210 seconds\n",
      "Differences for Cheek raiser is 0.036\n",
      "Differences for Lip corner puller is 0.033\n",
      "Differences for Brow lowerer is 0.026\n",
      "Differences for Lid tightener is 0.028\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 took 112.270 seconds\n",
      "Differences for Cheek raiser is 0.038\n",
      "Differences for Lip corner puller is 0.034\n",
      "Differences for Brow lowerer is 0.030\n",
      "Differences for Lid tightener is 0.030\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [01:45<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 took 113.037 seconds\n",
      "Differences for Cheek raiser is 0.037\n",
      "Differences for Lip corner puller is 0.034\n",
      "Differences for Brow lowerer is 0.029\n",
      "Differences for Lid tightener is 0.031\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "first_relevances = relevances\n",
    "\n",
    "for i in range(5):\n",
    "    start_time = time.time()\n",
    "    relevances = dianna.explain_image(model.run_on_batch, x, method=\"RISE\",\n",
    "                                      labels=[i for i in range(41)],\n",
    "                                      n_masks=1000, feature_res=6, p_keep=.1,\n",
    "                                      axis_labels={2: 'channels'}, batch_size=10)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Iteration {i} took {elapsed_time:.3f} seconds\")\n",
    "    # compare the MAE of each iteration compared to the first one above\n",
    "    for class_idx in prediction_ids:\n",
    "        diff_relevances = np.mean(np.abs(relevances[class_idx] - first_relevances[class_idx]))\n",
    "        print(f'Differences for {class_name(class_idx)} is {diff_relevances:.3f}')\n",
    "    print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9501549-aafa-4ad3-b0f5-295904752627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
